<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/mind-svgrepo-com.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Zeyi Huang</a><sup>1</sup><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Yuyang Ji</a><sup>2</sup><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Xiaofang Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Nikhil Mehta</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Tong Xiao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Donghyun Lee</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Sigmund Vanvalkenburgh</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Shengxin Zha</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Bolin Lai</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Licheng Yu</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ning Zhang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Yong Jae Lee</a><sup>2</sup><sup>†</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Miao Liu</a><sup>2</sup><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Wisconsin-Madison,</span>
            <span class="author-block"><sup>2</sup>Meta,</span>
            <span class="author-block"><sup>3</sup>UIUC</span>
          </div>

          <div class="is-size-5 has-text-weight-bold" style="color: red; margin-top: 0.5rem;">
            CVPR 2025
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
               <!-- demo Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-laptop-code"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Benchmark</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/teaser_8.png" alt="Teaser image" style="max-width: 100%; height: auto; border-radius: 12px;">
      <h2 class="subtitle has-text-centered" style="margin-top: 0.5rem; font-size: 0.95rem;">
        (Right) Our VideoMindPalace represents video data as a layered, topological structured graph, where nodes capture spatial concepts 
        (e.g., objects, activity zones, rooms), and edges signify spatiotemporal, layout relationships and human-object interaction. 
        This graph can be represented in JSON format and used as input to text-only LLMs. (Center) VideoTree
        extracts query-relevant information by organizing videos as tree structures, with deeper branches capturing finer, query-specific details. 
        A captioner then generates video descriptions from this structure, enabling the LLM to perform reasoning over long videos. 
        (Left) processes videos following temporal order, where visual captioners sequentially generate textual descriptions within each temporal sliding window, 
        which the LLM then aggregates for reasoning.
      </h2>
    </div>
  </div>
</section>

  
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Long-form video understanding with Large Vision Language Models is challenged by the need to analyze temporally dispersed yet spatially concentrated key moments
            within limited context windows. In this work, we introduce VideoMindPalace, a new framework inspired by the “Mind Palace”, which organizes critical video moments into a
            topologically structured semantic graph. VideoMindPalace organizes key information through (i) hand-object tracking and interaction, (ii) clustered activity zones representing specific areas of recurring activities, 
            and (iii) environment layout mapping, allowing natural language parsing by LLMs to provide grounded insights on spatio-temporal and 3D context. In addition, we propose the Video Mind Palace Benchmark (VMB), 
            to assess human-like reasoning, including spatial localization, temporal reasoning, and layout-aware sequential understanding. Evaluated on VMB and established video QA datasets, including EgoSchema,
            NExT-QA, IntentQA, and the Active Memories Benchmark, VideoMindPalace demonstrates notable gains in spatio temporal coherence and human-aligned reasoning, advancing long-form video analysis capabilities in VLMs.
          </p>
      
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!--     Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" style="padding-top: 4rem; padding-bottom: 4rem;">
  <div class="container is-max-desktop">
    <!-- Method Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-2" style="margin-bottom: 2rem;">Method</h2>
        <figure class="has-text-centered">
          <img src="./static/images/main_v5.png" 
               alt="Method diagram" 
               style="max-width: 100%; height: auto; border-radius: 16px; box-shadow: 0 6px 16px rgba(0,0,0,0.15);">
          <figcaption style="font-size: 1.0rem; margin-top: 1rem; line-height: 1.6; max-width: 900px; margin-left: auto; margin-right: auto;">
            Overview of our VideoMindPalace framework. 1) VideoMindPalace is a three-layered graph with nodes representing spatial concepts (e.g., objects, zones, rooms) and edges capturing spatiotemporal relationships. 
            Layer 1 - Human and Object: Nodes represent the human, and detected objects, with edges denoting spatiotemporal connections and interactions. 
            Layer 2 - Activity Zones: Nodes represent specific activity zones with edges showing 3D spatial relationships. 
            Layer 3 - Scene Layout: Nodes represent rooms with edges for relative distances. 2) This graph can be represented in the JSON format used as input to LLMs. 
            The model’s responses are grounded in the physical scene, enabling it to identify locations, locate items of interest, and understand the topological structure of the space.
          </figcaption>
        </figure>
      </div>
    </div>
    <!--/ Method Section -->
  </div>
</section>


<section class="section" style="padding-top: 4rem; padding-bottom: 4rem;">
  <div class="container is-max-desktop">
    <!-- Demo Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-2" style="margin-bottom: 2rem;">Video MindPalace Benchmark</h2>
        <figure class="has-text-centered">
          <img src="./static/images/results_7.png" 
               alt="Method diagram" 
               style="max-width: 100%; height: auto; border-radius: 16px; box-shadow: 0 6px 16px rgba(0,0,0,0.15);">
          <figcaption style="font-size: 1.0rem; margin-top: 1rem; line-height: 1.6; max-width: 900px; margin-left: auto; margin-right: auto;">
            Qualitative results of VideoMindPalace on the VMB benchmark, with an example for each question type. To explore how
VideoMindPalace successfully answers these questions, we prompt GPT-4 to identify the specific parts of the graph that provide sufficient
information to answer each question accurately.
          </figcaption>
        </figure>
      </div>
    </div>
    <!--/ Method Section -->
  </div>
</section>
  
<section class="section" style="padding-top: 4rem; padding-bottom: 4rem;">
  <div class="container is-max-desktop">
    <!-- Demo Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-2" style="margin-bottom: 2rem;">Demo</h2>
        <figure class="has-text-centered">
          <img src="./static/images/demo.png" 
               alt="Method diagram" 
               style="max-width: 100%; height: auto; border-radius: 16px; box-shadow: 0 6px 16px rgba(0,0,0,0.15);">
          <figcaption style="font-size: 1.0rem; margin-top: 1rem; line-height: 1.6; max-width: 900px; margin-left: auto; margin-right: auto;">
            Graph representation of a 30-second video using our Video MindPalace, which organizes information across three semantic layers.
          </figcaption>
        </figure>
      </div>
    </div>
    <!--/ Method Section -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{huang2025building,
  title={Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs},
  author={Huang, Zeyi and Ji, Yuyang and Wang, Xiaofang and Mehta, Nikhil and Xiao, Tong and Lee, Donghyun and Vanvalkenburgh, Sigmund and Zha, Shengxin and Lai, Bolin and Yu, Licheng and others},
  journal={arXiv preprint arXiv:2501.04336},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
<!--     <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered"> -->
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
